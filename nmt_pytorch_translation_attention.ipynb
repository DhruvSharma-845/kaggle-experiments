{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:29.257545Z","iopub.execute_input":"2025-10-22T19:41:29.257835Z","iopub.status.idle":"2025-10-22T19:41:29.263071Z","shell.execute_reply.started":"2025-10-22T19:41:29.257813Z","shell.execute_reply":"2025-10-22T19:41:29.262366Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"# Creating Model","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:29.267398Z","iopub.execute_input":"2025-10-22T19:41:29.267933Z","iopub.status.idle":"2025-10-22T19:41:29.280504Z","shell.execute_reply.started":"2025-10-22T19:41:29.267912Z","shell.execute_reply":"2025-10-22T19:41:29.279618Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# import torch.nn as nn\n\n# class Attention(nn.Module):\n#     def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n#         super().__init__()\n#         self.attn_fc = nn.Linear((encoder_hidden_dim * 2) + decoder_hidden_dim, decoder_hidden_dim)\n#         self.v_fc = nn.Linear(decoder_hidden_dim, 1, bias=False)\n\n#     def forward(self, decoder_hidden, encoder_outputs):\n#         # hidden = [batch size, decoder hidden dim]\n#         # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n#         batch_size = encoder_outputs.shape[1]\n#         src_length = encoder_outputs.shape[0]\n#         # repeat decoder hidden state src_length times\n#         hidden = decoder_hidden.unsqueeze(1).repeat(1, src_length, 1)\n#         encoder_outputs = encoder_outputs.permute(1, 0, 2)\n#         # hidden = [batch size, src length, decoder hidden dim]\n#         # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n#         energy = torch.tanh(self.attn_fc(torch.cat((hidden, encoder_outputs), dim=2)))\n#         # energy = [batch size, src length, decoder hidden dim]\n#         attention = self.v_fc(energy).squeeze(2)\n#         # attention = [batch size, src length]\n#         return torch.softmax(attention, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:29.281644Z","iopub.execute_input":"2025-10-22T19:41:29.281977Z","iopub.status.idle":"2025-10-22T19:41:29.295949Z","shell.execute_reply.started":"2025-10-22T19:41:29.281958Z","shell.execute_reply":"2025-10-22T19:41:29.295189Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"class EncoderDecoderRNNAttention(nn.Module):\n    def __init__(self, vocab_size, encoder_embed_size, encoder_hidden_size, encoder_dropout_p, decoder_embed_size, decoder_hidden_size):\n        super(EncoderDecoderRNNAttention, self).__init__()\n        self.encoder_embedding = nn.Embedding(vocab_size, encoder_embed_size)\n        self.encoder_rnn = nn.GRU(encoder_embed_size, encoder_hidden_size, batch_first=True) \n        self.dropout = nn.Dropout(encoder_dropout_p)\n\n        self.decoder_embedding = nn.Embedding(vocab_size, decoder_embed_size)\n        self.decoder_rnn = nn.GRU(decoder_embed_size, decoder_hidden_size, batch_first=True)\n        self.decoder_full = nn.Linear(2 * decoder_hidden_size, vocab_size)\n        \n    def forward(self, src, target):\n        encoder_output = self.encoder_embedding(src)\n        encoder_output = self.dropout(encoder_output)\n        encoder_outputs, encoder_hidden = self.encoder_rnn(encoder_output) #output = (N,Seq_Length,H_enc), hidden = (1,N,H_enc)\n            \n        decoder_output = self.decoder_embedding(target)\n        decoder_outputs, _ = self.decoder_rnn(decoder_output, encoder_hidden)\n\n        attn_output = self.attention(query=decoder_outputs, key=encoder_outputs, value=encoder_outputs)\n        combined_output = torch.cat((attn_output, decoder_outputs), dim=-1)\n        \n        return self.decoder_full(combined_output).permute(0, 2, 1)\n\n    def attention(self, query, key, value):\n        scores = query @ key.transpose(1, 2)  # [B, Lq, D] @ [B, D, Lk]= [B, Lq, Lk]\n        weights = torch.softmax(scores, dim=-1)  # [B, Lq, Lk]\n        return weights @ value  # [B, Lq, Lk] @ [B, Lk, Dv] = [B, Lq, Dv]\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:42:28.418710Z","iopub.execute_input":"2025-10-22T19:42:28.419047Z","iopub.status.idle":"2025-10-22T19:42:28.427161Z","shell.execute_reply.started":"2025-10-22T19:42:28.419022Z","shell.execute_reply":"2025-10-22T19:42:28.426334Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"# Cross Entropy\n# Input = (Batch_size,vocab_size, Seq_length)\n# Label = (Batch_size, Seq_Length)\nxentropy = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:29.317040Z","iopub.execute_input":"2025-10-22T19:41:29.317299Z","iopub.status.idle":"2025-10-22T19:41:29.333624Z","shell.execute_reply.started":"2025-10-22T19:41:29.317256Z","shell.execute_reply":"2025-10-22T19:41:29.332744Z"}},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":"# Preparing dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:29.334503Z","iopub.execute_input":"2025-10-22T19:41:29.334787Z","iopub.status.idle":"2025-10-22T19:41:29.347718Z","shell.execute_reply.started":"2025-10-22T19:41:29.334761Z","shell.execute_reply":"2025-10-22T19:41:29.346838Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"import requests \n\ndef download_data(url, save_path, chunk_size=128):\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    with open(save_path, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n    print(f\"Successfully downloaded {save_path}\")\n\ndownload_data(\"https://download.pytorch.org/tutorial/data.zip\", \"data.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:29.348423Z","iopub.execute_input":"2025-10-22T19:41:29.348644Z","iopub.status.idle":"2025-10-22T19:41:29.789508Z","shell.execute_reply.started":"2025-10-22T19:41:29.348627Z","shell.execute_reply":"2025-10-22T19:41:29.788683Z"}},"outputs":[{"name":"stdout","text":"Successfully downloaded data.zip\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"from zipfile import ZipFile\n\nextract_to_path = \"/kaggle/working/datase_final\"\n\nwith ZipFile(\"/kaggle/working/data.zip\", 'r') as zip_object:\n    zip_object.extractall(path=extract_to_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:29.791917Z","iopub.execute_input":"2025-10-22T19:41:29.792163Z","iopub.status.idle":"2025-10-22T19:41:29.881462Z","shell.execute_reply.started":"2025-10-22T19:41:29.792143Z","shell.execute_reply":"2025-10-22T19:41:29.880331Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/datase_final/data/eng-fra.txt\", sep='\\t')\nprint(df)\nprint(df.head())\nprint(df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:29.882220Z","iopub.execute_input":"2025-10-22T19:41:29.882501Z","iopub.status.idle":"2025-10-22T19:41:30.316237Z","shell.execute_reply.started":"2025-10-22T19:41:29.882480Z","shell.execute_reply":"2025-10-22T19:41:30.315353Z"}},"outputs":[{"name":"stdout","text":"                                                      Go.  \\\n0                                                    Run!   \n1                                                    Run!   \n2                                                    Wow!   \n3                                                   Fire!   \n4                                                   Help!   \n...                                                   ...   \n135836  A carbon footprint is the amount of carbon dio...   \n135837  Death is something that we're often discourage...   \n135838  Since there are usually multiple websites on a...   \n135839  If someone who doesn't know your background sa...   \n135840  It may be impossible to get a completely error...   \n\n                                                     Va !  \n0                                                 Cours !  \n1                                                Courez !  \n2                                              Ça alors !  \n3                                                Au feu !  \n4                                              À l'aide !  \n...                                                   ...  \n135836  Une empreinte carbone est la somme de pollutio...  \n135837  La mort est une chose qu'on nous décourage sou...  \n135838  Puisqu'il y a de multiples sites web sur chaqu...  \n135839  Si quelqu'un qui ne connaît pas vos antécédent...  \n135840  Il est peut-être impossible d'obtenir un Corpu...  \n\n[135841 rows x 2 columns]\n     Go.        Va !\n0   Run!     Cours !\n1   Run!    Courez !\n2   Wow!  Ça alors !\n3  Fire!    Au feu !\n4  Help!  À l'aide !\n                                                      Go.  \\\ncount                                              135841   \nunique                                              93548   \ntop     I can't tell you how happy I am that you've co...   \nfreq                                                   32   \n\n                             Va !  \ncount                      135841  \nunique                     129322  \ntop     Comment cela se peut-il ?  \nfreq                            9  \n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"import tokenizers\n\ndef train_eng_fra():  # a generator function to iterate over all training text\n    for index, row in df.iterrows():\n        yield row.iloc[0]\n        yield row.iloc[1]\n\nmax_length = 500\nvocab_size = 10_000\nnmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\nnmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\nnmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\nnmt_tokenizer.enable_truncation(max_length=max_length)\nnmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\nnmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\nnmt_tokenizer.train_from_iterator(train_eng_fra(), nmt_tokenizer_trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:30.317355Z","iopub.execute_input":"2025-10-22T19:41:30.317677Z","iopub.status.idle":"2025-10-22T19:41:38.981495Z","shell.execute_reply.started":"2025-10-22T19:41:30.317657Z","shell.execute_reply":"2025-10-22T19:41:38.980695Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"print(nmt_tokenizer.decode(nmt_tokenizer.encode(\"I like soccer\").ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:38.982263Z","iopub.execute_input":"2025-10-22T19:41:38.982525Z","iopub.status.idle":"2025-10-22T19:41:38.987244Z","shell.execute_reply.started":"2025-10-22T19:41:38.982507Z","shell.execute_reply":"2025-10-22T19:41:38.986554Z"}},"outputs":[{"name":"stdout","text":"I like soccer\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom collections import namedtuple\n\nfields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\nclass NmtPair(namedtuple(\"NmtPairBase\", fields)):\n    def to(self, device):\n        return NmtPair(self.src_token_ids.to(device), self.src_mask.to(device),\n                       self.tgt_token_ids.to(device), self.tgt_mask.to(device))\n\nseq_length = 40\n\nclass TextDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        src_text = self.df.iloc[idx, 0]\n        tgt_text = self.df.iloc[idx, 1]\n        tgt_text = f\"<s> {tgt_text} </s>\"\n\n        src_encodings = nmt_tokenizer.encode(src_text)\n        tgt_encodings = nmt_tokenizer.encode(tgt_text)\n        inputs = NmtPair(torch.tensor(src_encodings.ids), torch.tensor(src_encodings.attention_mask), \n                         torch.tensor(tgt_encodings.ids[:-1]), torch.tensor(tgt_encodings.attention_mask[:-1]))\n        labels = torch.tensor(tgt_encodings.ids[1:])\n        return inputs, labels\n    \n\ntrain_data = TextDataset(df.iloc[:100000, :])\nvalid_data = TextDataset(df.iloc[100000:120000, :])\ntest_data = TextDataset(df.iloc[120000:, :])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:38.988123Z","iopub.execute_input":"2025-10-22T19:41:38.988436Z","iopub.status.idle":"2025-10-22T19:41:39.008439Z","shell.execute_reply.started":"2025-10-22T19:41:38.988410Z","shell.execute_reply":"2025-10-22T19:41:39.007625Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"def collate_batch(batch):\n    src_ids, src_mask, tgt_ids, tgt_mask, label_ids = [], [], [], [], []\n    for _text, _label in batch:\n        src_ids.append(_text[0])\n        src_mask.append(_text[1])\n        tgt_ids.append(_text[2])\n        tgt_mask.append(_text[3])\n        label_ids.append(_label)\n    label_ids_tensor = nn.utils.rnn.pad_sequence(label_ids, batch_first=True)\n    src_ids_tensor = nn.utils.rnn.pad_sequence(src_ids, batch_first=True)\n    src_mask_tensor = nn.utils.rnn.pad_sequence(src_mask, batch_first=True)\n    tgt_ids_tensor = nn.utils.rnn.pad_sequence(tgt_ids, batch_first=True)\n    tgt_mask_tensor = nn.utils.rnn.pad_sequence(tgt_mask, batch_first=True)\n    inputs = NmtPair(src_ids_tensor, src_mask_tensor, tgt_ids_tensor, tgt_mask_tensor)\n    return inputs, label_ids_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:39.009422Z","iopub.execute_input":"2025-10-22T19:41:39.009647Z","iopub.status.idle":"2025-10-22T19:41:39.026793Z","shell.execute_reply.started":"2025-10-22T19:41:39.009628Z","shell.execute_reply":"2025-10-22T19:41:39.025835Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_batch)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_batch)\nprint(len(train_dataloader))\nprint(len(valid_dataloader))\nprint(len(test_dataloader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:41:39.029113Z","iopub.execute_input":"2025-10-22T19:41:39.029407Z","iopub.status.idle":"2025-10-22T19:41:39.050129Z","shell.execute_reply.started":"2025-10-22T19:41:39.029388Z","shell.execute_reply":"2025-10-22T19:41:39.049390Z"}},"outputs":[{"name":"stdout","text":"3125\n625\n496\n","output_type":"stream"}],"execution_count":75},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"from torch.optim import Adam\n\ndef train_epoch(dataloader, model, optimizer, criterion):\n\n    total_loss = 0\n    for data in dataloader:\n        input_tensor, label_tensor = data\n        input_tensor, label_tensor = input_tensor.to(device), label_tensor.to(device)\n\n        # print(f\"src {input_tensor[0].shape}, tgt  {input_tensor[2].shape} label {label_tensor.shape}\")\n\n        optimizer.zero_grad()\n        \n        pred = model(input_tensor[0], input_tensor[2])\n        # print(pred.shape)\n        \n        loss = criterion(pred, label_tensor)\n        loss.backward()\n\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\nmodel = EncoderDecoderRNNAttention(vocab_size, 256, 256, 0.1, 256, 256)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:42:33.731054Z","iopub.execute_input":"2025-10-22T19:42:33.731379Z","iopub.status.idle":"2025-10-22T19:42:33.835942Z","shell.execute_reply.started":"2025-10-22T19:42:33.731355Z","shell.execute_reply":"2025-10-22T19:42:33.835230Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"EncoderDecoderRNNAttention(\n  (encoder_embedding): Embedding(10000, 256)\n  (encoder_rnn): GRU(256, 256, batch_first=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (decoder_embedding): Embedding(10000, 256)\n  (decoder_rnn): GRU(256, 256, batch_first=True)\n  (decoder_full): Linear(in_features=512, out_features=10000, bias=True)\n)"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"def train(train_dataloader, model, n_epochs, criterion, learning_rate=0.001):\n    print_loss_total = 0  # Reset every print_every\n\n    optimizer = Adam(model.parameters(), lr = learning_rate)\n\n    for epoch in range(1, n_epochs + 1):\n        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n        print_loss_total += loss\n\n        if epoch % 2 == 0:\n            print_loss_avg = print_loss_total / 2\n            print_loss_total = 0\n            print('(%d %d%%) %.4f' % (epoch, epoch / n_epochs * 100, print_loss_avg))\n\n\ntrain(train_dataloader, model, 10, xentropy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:42:36.972539Z","iopub.execute_input":"2025-10-22T19:42:36.973127Z","iopub.status.idle":"2025-10-22T19:50:08.773511Z","shell.execute_reply.started":"2025-10-22T19:42:36.973102Z","shell.execute_reply":"2025-10-22T19:50:08.772592Z"}},"outputs":[{"name":"stdout","text":"(2 20%) 1.2371\n(4 40%) 0.6497\n(6 60%) 0.5106\n(8 80%) 0.4393\n(10 100%) 0.3980\n","output_type":"stream"}],"execution_count":80},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate(model, sentence, tokenizer, max_length = 50):\n    model.eval()\n\n    tgt_sentence = f\"<s>\"\n    index = 0\n    with torch.no_grad():\n        for index in range(max_length):\n            src = torch.tensor(tokenizer.encode(sentence).ids, device=device).view(1, -1)\n            tgt = torch.tensor(tokenizer.encode(tgt_sentence).ids, device=device).view(1, -1)\n\n            pred = model(src, tgt)\n\n            pred_token_ids = pred.argmax(dim=1)  # find the best token IDs\n            next_token_id = pred_token_ids[0, index]  # take the last token ID\n\n            next_token = tokenizer.id_to_token(next_token_id)\n            print(next_token)\n            tgt_sentence += \" \" + next_token\n            if next_token_id == 3:\n                break\n\n    return tgt_sentence\n\nprint(evaluate(model, \"beautiful\", nmt_tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T19:54:56.870880Z","iopub.execute_input":"2025-10-22T19:54:56.871561Z","iopub.status.idle":"2025-10-22T19:54:56.887478Z","shell.execute_reply.started":"2025-10-22T19:54:56.871534Z","shell.execute_reply":"2025-10-22T19:54:56.886534Z"}},"outputs":[{"name":"stdout","text":"beau\ncoucher\nou\nbien\n.\n</s>\n<s> beau coucher ou bien . </s>\n","output_type":"stream"}],"execution_count":89}]}