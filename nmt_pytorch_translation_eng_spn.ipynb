{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:51:17.212300Z","iopub.execute_input":"2025-10-16T17:51:17.213028Z","iopub.status.idle":"2025-10-16T17:51:17.217703Z","shell.execute_reply.started":"2025-10-16T17:51:17.212993Z","shell.execute_reply":"2025-10-16T17:51:17.217013Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from datasets import load_dataset\nnmt_dataset = load_dataset(\"Helsinki-NLP/opus_books\", \"en-es\")  # only if you trust the user\nsplit = nmt_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\nnmt_train_set_temp, nmt_test_set = split[\"train\"], split[\"test\"]\nsplit = nmt_train_set_temp.train_test_split(train_size=0.8, seed=42)\nnmt_train_set, nmt_valid_set = split[\"train\"], split[\"test\"]\n\nprint(nmt_train_set)\nprint(nmt_valid_set)\nprint(nmt_test_set)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:51:17.218897Z","iopub.execute_input":"2025-10-16T17:51:17.219193Z","iopub.status.idle":"2025-10-16T17:51:17.973626Z","shell.execute_reply.started":"2025-10-16T17:51:17.219172Z","shell.execute_reply":"2025-10-16T17:51:17.973027Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['id', 'translation'],\n    num_rows: 59820\n})\nDataset({\n    features: ['id', 'translation'],\n    num_rows: 14956\n})\nDataset({\n    features: ['id', 'translation'],\n    num_rows: 18694\n})\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"nmt_train_set[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:51:17.974280Z","iopub.execute_input":"2025-10-16T17:51:17.974618Z","iopub.status.idle":"2025-10-16T17:51:17.979535Z","shell.execute_reply.started":"2025-10-16T17:51:17.974599Z","shell.execute_reply":"2025-10-16T17:51:17.978857Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'id': '73782',\n 'translation': {'en': '\"Yes, sir,\" the captain replied, \"and if I have no hesitation in treading this polar soil, it\\'s because no human being until now has left a footprint here.\"',\n  'es': '-Sí, señor, en efecto -respondió el capitán-, y lo hago sin vacilación porque ningún ser humano ha plantado hasta ahora el pie en esta tierra del Polo.'}}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"import tokenizers\n\ndef train_eng_spa():  # a generator function to iterate over all training text\n    for pair in nmt_train_set[\"translation\"]:\n        yield pair[\"en\"]\n        yield pair[\"es\"]\n\nmax_length = 500\nvocab_size = 10_000\nnmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\nnmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\nnmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\nnmt_tokenizer.enable_truncation(max_length=max_length)\nnmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\nnmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\nnmt_tokenizer.train_from_iterator(train_eng_spa(), nmt_tokenizer_trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:51:17.981108Z","iopub.execute_input":"2025-10-16T17:51:17.981300Z","iopub.status.idle":"2025-10-16T17:51:23.976488Z","shell.execute_reply.started":"2025-10-16T17:51:17.981285Z","shell.execute_reply":"2025-10-16T17:51:23.975635Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"nmt_tokenizer.encode(\"I like soccer\").ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:51:23.977574Z","iopub.execute_input":"2025-10-16T17:51:23.978314Z","iopub.status.idle":"2025-10-16T17:51:23.983538Z","shell.execute_reply.started":"2025-10-16T17:51:23.978289Z","shell.execute_reply":"2025-10-16T17:51:23.982797Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[42, 652, 232, 66, 322]"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"print([nmt_tokenizer.id_to_token(id) for id in [42, 652, 232, 66, 322]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T18:44:56.947837Z","iopub.execute_input":"2025-10-16T18:44:56.948417Z","iopub.status.idle":"2025-10-16T18:44:56.952800Z","shell.execute_reply.started":"2025-10-16T18:44:56.948391Z","shell.execute_reply":"2025-10-16T18:44:56.952003Z"}},"outputs":[{"name":"stdout","text":"['I', 'like', 'so', 'c', 'cer']\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"from collections import namedtuple\nfrom torch.utils.data import DataLoader\n\nfields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\nclass NmtPair(namedtuple(\"NmtPairBase\", fields)):\n    def to(self, device):\n        return NmtPair(self.src_token_ids.to(device), self.src_mask.to(device),\n                       self.tgt_token_ids.to(device), self.tgt_mask.to(device))\n        \ndef nmt_collate_fn(batch):\n    src_texts = [pair['translation']['en'] for pair in batch]\n    tgt_texts = [f\"<s> {pair['translation']['es']} </s>\" for pair in batch]\n    src_encodings = nmt_tokenizer.encode_batch(src_texts)\n    tgt_encodings = nmt_tokenizer.encode_batch(tgt_texts)\n    src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n    tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n    src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n    tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n    inputs = NmtPair(src_token_ids, src_mask, tgt_token_ids[:, :-1], tgt_mask[:, :-1])\n    labels = tgt_token_ids[:, 1:]\n    return inputs, labels\n\nbatch_size = 32\nnmt_train_loader = DataLoader(nmt_train_set, batch_size=batch_size, collate_fn=nmt_collate_fn, shuffle=True)\nnmt_valid_loader = DataLoader(nmt_valid_set, batch_size=batch_size, collate_fn=nmt_collate_fn)\nnmt_test_loader = DataLoader(nmt_test_set, batch_size=batch_size, collate_fn=nmt_collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T18:45:01.932737Z","iopub.execute_input":"2025-10-16T18:45:01.933468Z","iopub.status.idle":"2025-10-16T18:45:01.940490Z","shell.execute_reply.started":"2025-10-16T18:45:01.933442Z","shell.execute_reply":"2025-10-16T18:45:01.939843Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T17:51:24.004624Z","iopub.execute_input":"2025-10-16T17:51:24.004887Z","iopub.status.idle":"2025-10-16T17:51:24.098717Z","shell.execute_reply.started":"2025-10-16T17:51:24.004860Z","shell.execute_reply":"2025-10-16T17:51:24.098022Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nclass NmtModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim=512, pad_id=0, hidden_dim=512, n_layers=2):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.encoder = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n        self.decoder = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n        self.output = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, pair):\n        src_embeddings = self.embed(pair.src_token_ids)\n        tgt_embeddings = self.embed(pair.tgt_token_ids)\n        src_lengths = pair.src_mask.sum(dim=1)\n        src_packed = pack_padded_sequence(src_embeddings, lengths=src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n        _, hidden_states = self.encoder(src_packed)\n        outputs, _ = self.decoder(tgt_embeddings, hidden_states)\n        return self.output(outputs).permute(0, 2, 1)\n\ntorch.manual_seed(42)\nvocab_size = nmt_tokenizer.get_vocab_size()\nnmt_model = NmtModel(vocab_size).to(device)\nprint(nmt_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T18:45:08.043006Z","iopub.execute_input":"2025-10-16T18:45:08.043576Z","iopub.status.idle":"2025-10-16T18:45:08.173516Z","shell.execute_reply.started":"2025-10-16T18:45:08.043552Z","shell.execute_reply":"2025-10-16T18:45:08.172908Z"}},"outputs":[{"name":"stdout","text":"NmtModel(\n  (embed): Embedding(10000, 512, padding_idx=0)\n  (encoder): GRU(512, 512, num_layers=2, batch_first=True)\n  (decoder): GRU(512, 512, num_layers=2, batch_first=True)\n  (output): Linear(in_features=512, out_features=10000, bias=True)\n)\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"xentropy = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = torch.optim.Adam(nmt_model.parameters(), lr=0.005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T18:45:11.130341Z","iopub.execute_input":"2025-10-16T18:45:11.130981Z","iopub.status.idle":"2025-10-16T18:45:11.135388Z","shell.execute_reply.started":"2025-10-16T18:45:11.130939Z","shell.execute_reply":"2025-10-16T18:45:11.134422Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"num_epochs = 2\ntorch.manual_seed(1)\nfor epoch in range(num_epochs):\n    nmt_model.train()\n    running_loss = 0.0\n    idx = 0\n    for seq_batch_pair, target_batch in nmt_train_loader:\n        seq_batch_pair, target_batch = seq_batch_pair.to(device), target_batch.to(device)\n        \n        optimizer.zero_grad()\n        pred_logits = nmt_model(seq_batch_pair)\n        loss = xentropy(pred_logits, target_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        idx += 1\n        if idx >= 500:\n            break\n    \n    epoch_loss = running_loss / len(nmt_train_loader)\n    print(f'Epoch {epoch} loss: {loss:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T18:46:31.689178Z","iopub.execute_input":"2025-10-16T18:46:31.689678Z","iopub.status.idle":"2025-10-16T18:48:23.075231Z","shell.execute_reply.started":"2025-10-16T18:46:31.689657Z","shell.execute_reply":"2025-10-16T18:48:23.074346Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Epoch 0 loss: 5.4754\nEpoch 1 loss: nan\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"def translate(model, src_text, max_length=20, pad_id=0, eos_id=3):\n    tgt_text = \"\"\n    token_ids = []\n    for index in range(max_length):\n        batch, _ = nmt_collate_fn([{\"translation\": {\"en\": src_text, \"es\": tgt_text}}])\n        with torch.no_grad():\n            Y_logits = model(batch.to(device))\n            Y_token_ids = Y_logits.argmax(dim=1)  # find the best token IDs\n            next_token_id = Y_token_ids[0, index]  # take the last token ID\n\n        next_token = nmt_tokenizer.id_to_token(next_token_id)\n        tgt_text += \" \" + next_token\n        if next_token_id == eos_id:\n            break\n    return tgt_text\n\nnmt_model.eval()\nprint(translate(nmt_model, \"I like soccer.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-16T18:49:29.043913Z","iopub.execute_input":"2025-10-16T18:49:29.044213Z","iopub.status.idle":"2025-10-16T18:49:29.090578Z","shell.execute_reply.started":"2025-10-16T18:49:29.044192Z","shell.execute_reply":"2025-10-16T18:49:29.089980Z"}},"outputs":[{"name":"stdout","text":" <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","output_type":"stream"}],"execution_count":52}]}