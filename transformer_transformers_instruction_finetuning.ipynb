{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:46:08.051815Z","iopub.execute_input":"2025-11-29T06:46:08.052385Z","iopub.status.idle":"2025-11-29T06:46:08.353662Z","shell.execute_reply.started":"2025-11-29T06:46:08.052359Z","shell.execute_reply":"2025-11-29T06:46:08.352914Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade bitsandbytes peft trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:46:08.354946Z","iopub.execute_input":"2025-11-29T06:46:08.355291Z","iopub.status.idle":"2025-11-29T06:46:12.103651Z","shell.execute_reply.started":"2025-11-29T06:46:08.355272Z","shell.execute_reply":"2025-11-29T06:46:12.102931Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.48.2)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.18.0)\nRequirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.25.1)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.1.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.57.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.36.0)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.1)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load and format the data using the template TinyLLama is using\ndataset = (\n    load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft\")\n      .shuffle(seed=42)\n      .select(range(3_000))\n)\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:46:12.104578Z","iopub.execute_input":"2025-11-29T06:46:12.104858Z","iopub.status.idle":"2025-11-29T06:46:15.410405Z","shell.execute_reply.started":"2025-11-29T06:46:12.104825Z","shell.execute_reply":"2025-11-29T06:46:15.409259Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['prompt', 'prompt_id', 'messages'],\n    num_rows: 3000\n})\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(dataset['messages'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:46:15.411158Z","iopub.execute_input":"2025-11-29T06:46:15.411545Z","iopub.status.idle":"2025-11-29T06:46:15.420743Z","shell.execute_reply.started":"2025-11-29T06:46:15.411522Z","shell.execute_reply":"2025-11-29T06:46:15.419998Z"}},"outputs":[{"name":"stdout","text":"[{'content': \"Write a compelling mystery story set in a vineyard, where a seasoned detective investigates a murder with twists and turns that will keep the reader engaged until the very end. Add complex characters, multiple suspects, and red herrings to create suspense and challenge the detective's deductive reasoning. Use vivid descriptive language to paint a picture of the vineyard setting, its wine-making process, and the people who live and work there. Make sure to reveal clues and motives gradually, and create a satisfying resolution that ties up all loose ends.\", 'role': 'user'}, {'content': \"Detective Jameson had been called to the vineyard on the outskirts of town to investigate a murder. The sun was setting, casting long shadows over the grape vines, and the air was heavy with the sweet scent of fermented grapes. The body was lying in the middle of the vineyard, surrounded by broken grape vines and a scattering of grapes. The victim's throat had been slashed, and there were bruises around the neck.\\n\\nAs Jameson began his investigation, he met the owner of the vineyard, a tall, thin man with a stern face. The owner didn't seem overly concerned about the murder on his property and was reluctant to answer any questions that Jameson had. He was also dismissive of the other workers, including the winemaker, who appeared visibly distraught.\\n\\nJameson soon discovered that the victim, a young woman named Sophia, had been working at the vineyard for the past year. Sophia was well-liked by the other workers, who had nothing but positive things to say about her. She was hardworking and always had a smile on her face. As Jameson dug deeper, he found that Sophia had possibly been having an affair with the owner, who was married. The owner denied any involvement, but Jameson was suspicious, especially when he found a note in Sophia's pocket that hinted at a secret relationship.\\n\\nFurther investigation uncovered more potential suspects—a worker who had been fired earlier that day, a jealous ex-boyfriend, and even a rival vineyard owner. With so many suspects, Jameson had to be careful, and he wasn't sure who he could trust. The winemaker, who had initially seemed distraught, also didn't seem to be telling Jameson everything he knew.\\n\\nAs Jameson pieced together the clues, he found that the murder had been premeditated. The killer had used the grape vines to hide their tracks, and the bruises on Sophia's neck indicated she had been strangled before her throat was slashed. Jameson knew he was getting closer to solving the case, and he sensed the killer was getting nervous.\\n\\nThe final twist came when the winemaker confessed to the murder. He had been in love with Sophia and had become jealous when he discovered she had been seeing the owner. He had planned the murder carefully, knowing that Sophia had been working alone that day. Jameson arrested the winemaker, relieved that he had solved the case and that justice would be served.\\n\\nAs the sun rose over the vineyard the next day, the workers returned to their routine of harvesting grapes, but Jameson knew that the tranquility of the vineyard had been shattered by a brutal murder.\", 'role': 'assistant'}, {'content': 'Wow, that was a great mystery story! I really enjoyed the twists and turns, and the characters were interesting. Can you give me more detailed descriptions of the vineyard setting and the wine-making process? I want to be fully immersed in the story.', 'role': 'user'}, {'content': 'Of course! As Detective Jameson arrived at the vineyard, he was struck by the natural beauty of the surroundings. The rolling hills were covered in rows upon rows of grapevines, their leaves rustling gently in the breeze. The vineyard was divided into different sections, each one dedicated to a specific type of grape. Jameson could see the giant barrels inside the winery, where the grapes would be crushed, fermented, and aged until they became a fine wine.\\n\\nThe grapes themselves were a sight to behold—some were deep burgundy, others were a bright green, and some were shades in between. As Jameson walked through the vineyard, he could smell the sweet aroma of grapes hanging heavily from the vines. He saw workers picking the grapes carefully, placing them in baskets and moving them to the winery for processing. The winery itself was a sprawling building with large metal tanks, wooden barrels, and machinery scattered throughout the space.\\n\\nJameson had always been fascinated by the art of winemaking. He knew it was a delicate process that required patience, knowledge, and skill. It was a precise balance of science and art—the grapes had to be harvested at the right time, crushed in the right way, and aged properly to produce the perfect wine. As he learned more about the wine-making process, Jameson became convinced that the careful attention to detail could uncover critical clues to solve the murder mystery.\\n\\nThroughout his investigation, Jameson noticed other aspects of the vineyard that added to its charm—the sprawling estate with its mustard yellow stucco walls, the vintage bicycles parked near the winery, the creaky wooden tasting room, and the buzzing of bees that drifted through the vineyards.\\n\\nAll these sights and smells contributed to the peaceful setting of the vineyard, making it even more shocking when Jameson stumbled upon a gruesome murder. The picturesque vineyard was now a crime scene, and Jameson knew that he had his work cut out for him to catch the killer.', 'role': 'assistant'}, {'content': \"That was an amazing description! It really transported me into the vineyard with Detective Jameson. Can you tell me more about the different types of grapes that were grown in the vineyard? I'm curious to know which ones were used to create the murder victim's wine.\", 'role': 'user'}, {'content': \"Yes, the vineyard grew various types of grapes, each with its unique flavor profile and characteristics. The vineyard had vines of red grapes, such as Pinot Noir, Cabernet Franc, Cabernet Sauvignon, and Merlot. These grapes added depth, structure, and complexity to the wine, and were often blended to create different red wine blends.\\n\\nThe vineyard also had vines of white grapes, such as Chardonnay, Sauvignon Blanc, and Viognier. These grapes were used to produce white wine, which was light, refreshing, and crisp. Viognier, in particular, was known to be used to create a small batch, highly sought-after white wine from the vineyard.\\n\\nSophia, the victim, was known to have been a specialist in selecting the perfect blend of grapes for each bottle of wine. She had a unique talent for detecting the subtle flavors and nuances of each grape and was often tasked with creating the vineyard's signature wines.\\n\\nWhen Detective Jameson examined the grape vines around the scene of the crime, he noticed that some of the vines had been uprooted and damaged. He also saw that the grapes in the area were Merlot grapes, which were used to create the vineyard's award-winning red wine.\\n\\nAs Jameson dove deeper into the investigation, he uncovered a plot to sabotage the Merlot grapes. Someone had deliberately damaged the vines, destroying the grapes' flavor and texture, and ultimately ruining the wine. Sophia had been investigating the sabotage, and that could have led to her murder.\\n\\nDetective Jameson quickly realized that the Merlot grapes were a crucial clue in the murder investigation. He interviewed every worker present at the vineyard, looked through the vineyard's production logs, and analyzed samples of each grape variety to identify potential suspects. The hunt for the killer led Jameson on a roller-coaster ride of clues and red herrings, and ultimately to the arrest of the winemaker.\", 'role': 'assistant'}]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer\n# Load a tokenizer to use its chat template\ntemplate_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n\ndef format_prompt(example):\n    chat = example[\"messages\"]\n    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n\n    return {\"text\": prompt}\n    \ndataset = dataset.map(format_prompt, remove_columns=dataset.column_names)\nprint(dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:53:55.028894Z","iopub.execute_input":"2025-11-29T06:53:55.029636Z","iopub.status.idle":"2025-11-29T06:53:55.861617Z","shell.execute_reply.started":"2025-11-29T06:53:55.029609Z","shell.execute_reply":"2025-11-29T06:53:55.860757Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"190cceeb45ce45af96f98b64029c63eb"}},"metadata":{}},{"name":"stdout","text":"{'text': \"<|user|>\\nWrite a compelling mystery story set in a vineyard, where a seasoned detective investigates a murder with twists and turns that will keep the reader engaged until the very end. Add complex characters, multiple suspects, and red herrings to create suspense and challenge the detective's deductive reasoning. Use vivid descriptive language to paint a picture of the vineyard setting, its wine-making process, and the people who live and work there. Make sure to reveal clues and motives gradually, and create a satisfying resolution that ties up all loose ends.</s>\\n<|assistant|>\\nDetective Jameson had been called to the vineyard on the outskirts of town to investigate a murder. The sun was setting, casting long shadows over the grape vines, and the air was heavy with the sweet scent of fermented grapes. The body was lying in the middle of the vineyard, surrounded by broken grape vines and a scattering of grapes. The victim's throat had been slashed, and there were bruises around the neck.\\n\\nAs Jameson began his investigation, he met the owner of the vineyard, a tall, thin man with a stern face. The owner didn't seem overly concerned about the murder on his property and was reluctant to answer any questions that Jameson had. He was also dismissive of the other workers, including the winemaker, who appeared visibly distraught.\\n\\nJameson soon discovered that the victim, a young woman named Sophia, had been working at the vineyard for the past year. Sophia was well-liked by the other workers, who had nothing but positive things to say about her. She was hardworking and always had a smile on her face. As Jameson dug deeper, he found that Sophia had possibly been having an affair with the owner, who was married. The owner denied any involvement, but Jameson was suspicious, especially when he found a note in Sophia's pocket that hinted at a secret relationship.\\n\\nFurther investigation uncovered more potential suspects—a worker who had been fired earlier that day, a jealous ex-boyfriend, and even a rival vineyard owner. With so many suspects, Jameson had to be careful, and he wasn't sure who he could trust. The winemaker, who had initially seemed distraught, also didn't seem to be telling Jameson everything he knew.\\n\\nAs Jameson pieced together the clues, he found that the murder had been premeditated. The killer had used the grape vines to hide their tracks, and the bruises on Sophia's neck indicated she had been strangled before her throat was slashed. Jameson knew he was getting closer to solving the case, and he sensed the killer was getting nervous.\\n\\nThe final twist came when the winemaker confessed to the murder. He had been in love with Sophia and had become jealous when he discovered she had been seeing the owner. He had planned the murder carefully, knowing that Sophia had been working alone that day. Jameson arrested the winemaker, relieved that he had solved the case and that justice would be served.\\n\\nAs the sun rose over the vineyard the next day, the workers returned to their routine of harvesting grapes, but Jameson knew that the tranquility of the vineyard had been shattered by a brutal murder.</s>\\n<|user|>\\nWow, that was a great mystery story! I really enjoyed the twists and turns, and the characters were interesting. Can you give me more detailed descriptions of the vineyard setting and the wine-making process? I want to be fully immersed in the story.</s>\\n<|assistant|>\\nOf course! As Detective Jameson arrived at the vineyard, he was struck by the natural beauty of the surroundings. The rolling hills were covered in rows upon rows of grapevines, their leaves rustling gently in the breeze. The vineyard was divided into different sections, each one dedicated to a specific type of grape. Jameson could see the giant barrels inside the winery, where the grapes would be crushed, fermented, and aged until they became a fine wine.\\n\\nThe grapes themselves were a sight to behold—some were deep burgundy, others were a bright green, and some were shades in between. As Jameson walked through the vineyard, he could smell the sweet aroma of grapes hanging heavily from the vines. He saw workers picking the grapes carefully, placing them in baskets and moving them to the winery for processing. The winery itself was a sprawling building with large metal tanks, wooden barrels, and machinery scattered throughout the space.\\n\\nJameson had always been fascinated by the art of winemaking. He knew it was a delicate process that required patience, knowledge, and skill. It was a precise balance of science and art—the grapes had to be harvested at the right time, crushed in the right way, and aged properly to produce the perfect wine. As he learned more about the wine-making process, Jameson became convinced that the careful attention to detail could uncover critical clues to solve the murder mystery.\\n\\nThroughout his investigation, Jameson noticed other aspects of the vineyard that added to its charm—the sprawling estate with its mustard yellow stucco walls, the vintage bicycles parked near the winery, the creaky wooden tasting room, and the buzzing of bees that drifted through the vineyards.\\n\\nAll these sights and smells contributed to the peaceful setting of the vineyard, making it even more shocking when Jameson stumbled upon a gruesome murder. The picturesque vineyard was now a crime scene, and Jameson knew that he had his work cut out for him to catch the killer.</s>\\n<|user|>\\nThat was an amazing description! It really transported me into the vineyard with Detective Jameson. Can you tell me more about the different types of grapes that were grown in the vineyard? I'm curious to know which ones were used to create the murder victim's wine.</s>\\n<|assistant|>\\nYes, the vineyard grew various types of grapes, each with its unique flavor profile and characteristics. The vineyard had vines of red grapes, such as Pinot Noir, Cabernet Franc, Cabernet Sauvignon, and Merlot. These grapes added depth, structure, and complexity to the wine, and were often blended to create different red wine blends.\\n\\nThe vineyard also had vines of white grapes, such as Chardonnay, Sauvignon Blanc, and Viognier. These grapes were used to produce white wine, which was light, refreshing, and crisp. Viognier, in particular, was known to be used to create a small batch, highly sought-after white wine from the vineyard.\\n\\nSophia, the victim, was known to have been a specialist in selecting the perfect blend of grapes for each bottle of wine. She had a unique talent for detecting the subtle flavors and nuances of each grape and was often tasked with creating the vineyard's signature wines.\\n\\nWhen Detective Jameson examined the grape vines around the scene of the crime, he noticed that some of the vines had been uprooted and damaged. He also saw that the grapes in the area were Merlot grapes, which were used to create the vineyard's award-winning red wine.\\n\\nAs Jameson dove deeper into the investigation, he uncovered a plot to sabotage the Merlot grapes. Someone had deliberately damaged the vines, destroying the grapes' flavor and texture, and ultimately ruining the wine. Sophia had been investigating the sabotage, and that could have led to her murder.\\n\\nDetective Jameson quickly realized that the Merlot grapes were a crucial clue in the murder investigation. He interviewed every worker present at the vineyard, looked through the vineyard's production logs, and analyzed samples of each grape variety to identify potential suspects. The hunt for the killer led Jameson on a roller-coaster ride of clues and red herrings, and ultimately to the arrest of the winemaker.</s>\\n\"}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Applying Quantization","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# 4-bit quantization configuration - Q in QLoRA\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Use 4-bit precision model loading\n    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n)\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n\n# Load the model to train on the GPU\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = \"<PAD>\"\ntokenizer.padding_side = \"left\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:54:00.776612Z","iopub.execute_input":"2025-11-29T06:54:00.777157Z","iopub.status.idle":"2025-11-29T06:54:03.782631Z","shell.execute_reply.started":"2025-11-29T06:54:00.777133Z","shell.execute_reply":"2025-11-29T06:54:03.781918Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# Prepare LoRA Configuration\npeft_config = LoraConfig(\n    lora_alpha=32,  # LoRA Scaling\n    lora_dropout=0.1,  # Dropout for LoRA Layers\n    r=64,  # Rank\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=  # Layers to target\n     [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"]\n)\n\n# Prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:54:10.246193Z","iopub.execute_input":"2025-11-29T06:54:10.246765Z","iopub.status.idle":"2025-11-29T06:54:11.415683Z","shell.execute_reply.started":"2025-11-29T06:54:10.246743Z","shell.execute_reply":"2025-11-29T06:54:11.414851Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom trl import SFTTrainer\n\noutput_dir = \"./results\"\n\n# Training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    num_train_epochs=1,\n    logging_steps=10,\n    fp16=True,\n    gradient_checkpointing=True,\n    report_to=\"none\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    args=training_arguments,\n    peft_config=peft_config,\n)\n\n# Train model\ntrainer.train()\n\n# Save QLoRA weights\ntrainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T06:56:26.012103Z","iopub.execute_input":"2025-11-29T06:56:26.012849Z","iopub.status.idle":"2025-11-29T08:07:19.751848Z","shell.execute_reply.started":"2025-11-29T06:56:26.012822Z","shell.execute_reply":"2025-11-29T08:07:19.751197Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 1:10:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.561900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.383700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.315100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.335000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.350500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.276700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.374400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.352800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.306900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.293400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.272100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.277000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.239400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.350000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.274900</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.300000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.323800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.224400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.308000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.303600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.324600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.222300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.280300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.306200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.234600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.214100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.300000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.311100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.302600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.273000</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.272400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.306400</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.246300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.254400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.231500</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.315900</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.344800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"TinyLlama-1.1B-qlora\",\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)\n\n# Merge LoRA and base model\nmerged_model = model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:09:14.833182Z","iopub.execute_input":"2025-11-29T08:09:14.834028Z","iopub.status.idle":"2025-11-29T08:09:16.897548Z","shell.execute_reply.started":"2025-11-29T08:09:14.833999Z","shell.execute_reply":"2025-11-29T08:09:16.896904Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import pipeline\n\n# Use our predefined prompt template\nprompt = \"\"\"<|user|>\nTell me something about Large Language Models.</s>\n<|assistant|>\n\"\"\"\n\n# Run our instruction-tuned model\npipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\nprint(pipe(prompt)[0][\"generated_text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T08:09:21.487672Z","iopub.execute_input":"2025-11-29T08:09:21.488311Z","iopub.status.idle":"2025-11-29T08:09:27.585726Z","shell.execute_reply.started":"2025-11-29T08:09:21.488281Z","shell.execute_reply":"2025-11-29T08:09:27.585125Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"<|user|>\nTell me something about Large Language Models.</s>\n<|assistant|>\nLarge language models are a type of artificial intelligence (AI) that can learn to generate text from given inputs using deep neural networks. These models have been developed by researchers from different companies and universities around the world. They are capable of understanding language and generating new text using a variety of languages.\n\nLarge Language Models have the ability to generate text in a variety of styles, such as natural language processing (NLP), machine translation (MT), and question answering (QA). These models can also learn from data and improve their abilities over time. Large Language Models are usually trained on large volumes of data, which can be provided by text or images.\n\nLarge Language Models can be used to generate content for many applications, such as chatbots, search engines, and news articles. They are also being used to generate text for text-to-speech and speech-to-text systems, as well as for natural language processing.\n\nSome notable examples of Large Language Models include GPT-3, GPT-2, BERT, and XLM-R.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Preference Tuning using DPO","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Apply formatting to the dataset and select relatively short answers\ndpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\ndpo_dataset = dpo_dataset.filter(\n    lambda r: \n        r[\"status\"] != \"tie\" and \n        r[\"chosen_score\"] >= 8 and \n        not r[\"in_gsm8k_train\"]\n)\nprint(dpo_dataset[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_prompt(example):\n    \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\"\n\n    # Format answers\n    system = \"<|system|>\\n\" + example[\"system\"] + \"</s>\\n\"\n    prompt = \"<|user|>\\n\" + example[\"input\"] + \"</s>\\n<|assistant|>\\n\"\n    chosen = example[\"chosen\"] + \"</s>\\n\"\n    rejected = example[\"rejected\"] + \"</s>\\n\"\n\n    return {\n        \"prompt\": system + prompt,\n        \"chosen\": chosen,\n        \"rejected\": rejected,\n    }\ndpo_dataset = dpo_dataset.map(\n    format_prompt,  remove_columns=dpo_dataset.column_names\n)\nprint(dpo_dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\nfrom transformers import BitsAndBytesConfig, AutoTokenizer\n\n# 4-bit quantization configuration - Q in QLoRA\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Use 4-bit precision model loading\n    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n)\n\n# Merge LoRA and base model\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"TinyLlama-1.1B-qlora\",\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n    quantization_config=bnb_config,\n)\nmerged_model = model.merge_and_unload()\n\n# Load LLaMA tokenizer\nmodel_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = \"<PAD>\"\ntokenizer.padding_side = \"left\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\n# Prepare LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=32,  # LoRA Scaling\n    lora_dropout=0.1,  # Dropout for LoRA Layers\n    r=64,  # Rank\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=  # Layers to target\n     [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"]\n)\n\n# prepare model for training\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import DPOConfig, DPOTrainer\n\n\noutput_dir = \"./results\"\n\n# Training arguments\ntraining_arguments = DPOConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    learning_rate=1e-5,\n    lr_scheduler_type=\"cosine\",\n    max_steps=200,\n    logging_steps=10,\n    fp16=True,\n    gradient_checkpointing=True,\n    warmup_ratio=0.1,\n    report_to=\"none\"\n)\n\n# Create DPO trainer\ndpo_trainer = DPOTrainer(\n    model,\n    args=training_arguments,\n    train_dataset=dpo_dataset,\n    tokenizer=tokenizer,\n    peft_config=peft_config,\n    beta=0.1,\n    max_prompt_length=512,\n    max_length=512,\n)\n\n# Fine-tune model with DPO\ndpo_trainer.train()\n\n# Save adapter\ndpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\n\n# Merge LoRA and base model\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n    \"TinyLlama-1.1B-qlora\",\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)\nsft_model = model.merge_and_unload()\n\n# Merge DPO LoRA and SFT model\ndpo_model = PeftModel.from_pretrained(\n    sft_model,\n    \"TinyLlama-1.1B-dpo-qlora\",\n    device_map=\"auto\",\n)\ndpo_model = dpo_model.merge_and_unload()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}