{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T18:02:27.121377Z","iopub.execute_input":"2025-10-15T18:02:27.121547Z","iopub.status.idle":"2025-10-15T18:02:28.305608Z","shell.execute_reply.started":"2025-10-15T18:02:27.121532Z","shell.execute_reply":"2025-10-15T18:02:28.304947Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!curl -O https://www.gutenberg.org/files/1268/1268-0.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T18:02:36.674444Z","iopub.execute_input":"2025-10-15T18:02:36.674999Z","iopub.status.idle":"2025-10-15T18:02:37.980538Z","shell.execute_reply.started":"2025-10-15T18:02:36.674975Z","shell.execute_reply":"2025-10-15T18:02:37.979839Z"}},"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1124k  100 1124k    0     0  1039k      0  0:00:01  0:00:01 --:--:-- 1039k\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pathlib import Path\nimport urllib.request\n\ndef download_shakespeare_text():\n    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n    if not path.is_file():\n        path.parent.mkdir(parents=True, exist_ok=True)\n        url = \"https://homl.info/shakespeare\"\n        urllib.request.urlretrieve(url, path)\n    return path.read_text()\n\nshakespeare_text = download_shakespeare_text()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:14:35.873737Z","iopub.execute_input":"2025-10-15T19:14:35.874433Z","iopub.status.idle":"2025-10-15T19:14:36.499897Z","shell.execute_reply.started":"2025-10-15T19:14:35.874406Z","shell.execute_reply":"2025-10-15T19:14:36.499094Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"text = shakespeare_text\nchar_set = set(text)\nprint('Total Length:', len(text))\nprint('Unique Characters:', len(char_set))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:15:53.782060Z","iopub.execute_input":"2025-10-15T19:15:53.782390Z","iopub.status.idle":"2025-10-15T19:15:53.796223Z","shell.execute_reply.started":"2025-10-15T19:15:53.782369Z","shell.execute_reply":"2025-10-15T19:15:53.795467Z"}},"outputs":[{"name":"stdout","text":"Total Length: 1115394\nUnique Characters: 65\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import numpy as np\nwith open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n    text = fp.read()\nstart_indx = text.find('THE MYSTERIOUS ISLAND')\nend_indx = text.find('End of the Project Gutenberg')\ntext = text[start_indx:end_indx]\nchar_set = set(text)\nprint('Total Length:', len(text))\nprint('Unique Characters:', len(char_set))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T18:40:15.989088Z","iopub.execute_input":"2025-10-15T18:40:15.989722Z","iopub.status.idle":"2025-10-15T18:40:16.012841Z","shell.execute_reply.started":"2025-10-15T18:40:15.989692Z","shell.execute_reply":"2025-10-15T18:40:16.012251Z"}},"outputs":[{"name":"stdout","text":"Total Length: 1112310\nUnique Characters: 80\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"chars_sorted = sorted(char_set)\nprint(\"\".join(chars_sorted))\nchar2int = {ch:i for i, ch in enumerate(chars_sorted)}\nchar_array = np.array(chars_sorted)\ntext_encoded = torch.tensor([char2int[ch] for ch in text])\nprint('Text encoded shape:', text_encoded.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:16:04.995678Z","iopub.execute_input":"2025-10-15T19:16:04.996353Z","iopub.status.idle":"2025-10-15T19:16:05.210082Z","shell.execute_reply.started":"2025-10-15T19:16:04.996328Z","shell.execute_reply":"2025-10-15T19:16:05.209459Z"}},"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nText encoded shape: torch.Size([1115394])\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nseq_length = 40\n\nclass TextDataset(Dataset):\n    def __init__(self, encoded_text, chunk_size):\n        self.encoded_text = encoded_text\n        self.chunk_size = chunk_size\n\n    def __len__(self):\n        return len(self.encoded_text) - self.chunk_size\n\n    def __getitem__(self, idx):\n        #text_chunk = torch.from_numpy(self.encoded_text[idx: idx + self.chunk_size])\n        #return text_chunk[:-1].long(), text_chunk[1:].long()\n        end = idx + self.chunk_size\n        window = self.encoded_text[idx : end]\n        target = self.encoded_text[idx + 1 : end + 1]\n        return window, target\n\nseq_dataset = TextDataset(text_encoded, seq_length)\nprint(len(seq_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:16:29.916215Z","iopub.execute_input":"2025-10-15T19:16:29.916467Z","iopub.status.idle":"2025-10-15T19:16:29.922219Z","shell.execute_reply.started":"2025-10-15T19:16:29.916450Z","shell.execute_reply":"2025-10-15T19:16:29.921451Z"}},"outputs":[{"name":"stdout","text":"1115354\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nbatch_size = 64\ntorch.manual_seed(1)\nseq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\nprint(seq_dl)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:16:34.374216Z","iopub.execute_input":"2025-10-15T19:16:34.374973Z","iopub.status.idle":"2025-10-15T19:16:34.380415Z","shell.execute_reply.started":"2025-10-15T19:16:34.374942Z","shell.execute_reply":"2025-10-15T19:16:34.379783Z"}},"outputs":[{"name":"stdout","text":"<torch.utils.data.dataloader.DataLoader object at 0x7d1c09b1f190>\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:16:37.290396Z","iopub.execute_input":"2025-10-15T19:16:37.290649Z","iopub.status.idle":"2025-10-15T19:16:37.294818Z","shell.execute_reply.started":"2025-10-15T19:16:37.290630Z","shell.execute_reply":"2025-10-15T19:16:37.294054Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"import torch.nn as nn\nclass charRNN(nn.Module):\n    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, n_layers=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn_hidden_size = rnn_hidden_size\n        self.rnn = nn.GRU(embed_dim, rnn_hidden_size, num_layers=n_layers, batch_first=True)\n        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n\n    def forward(self, x):\n        embeddings = self.embedding(x)\n        out, _states = self.rnn(embeddings)\n        return self.fc(out).permute(0, 2, 1)\n\n    # def init_hidden(self, batch_size):\n    #     hidden = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n    #     cell = torch.zeros(1, batch_size, self.rnn_hidden_size).to(device)\n    #     return hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:16:40.582087Z","iopub.execute_input":"2025-10-15T19:16:40.582785Z","iopub.status.idle":"2025-10-15T19:16:40.587579Z","shell.execute_reply.started":"2025-10-15T19:16:40.582760Z","shell.execute_reply":"2025-10-15T19:16:40.586834Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"vocab_size = len(char_set)\nembed_dim = 256\nrnn_hidden_size = 512\ntorch.manual_seed(1)\nmodel = charRNN(vocab_size, embed_dim, rnn_hidden_size)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:16:43.831904Z","iopub.execute_input":"2025-10-15T19:16:43.832251Z","iopub.status.idle":"2025-10-15T19:16:43.861451Z","shell.execute_reply.started":"2025-10-15T19:16:43.832228Z","shell.execute_reply":"2025-10-15T19:16:43.860839Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"charRNN(\n  (embedding): Embedding(65, 256)\n  (rnn): GRU(256, 512, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=512, out_features=65, bias=True)\n)"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:16:46.799334Z","iopub.execute_input":"2025-10-15T19:16:46.799644Z","iopub.status.idle":"2025-10-15T19:16:46.803855Z","shell.execute_reply.started":"2025-10-15T19:16:46.799627Z","shell.execute_reply":"2025-10-15T19:16:46.802984Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"num_epochs = 10\ntorch.manual_seed(1)\nfor epoch in range(num_epochs):\n    idx = 0\n    for seq_batch, target_batch in seq_dl:\n        seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)\n        \n        optimizer.zero_grad()\n        loss = 0\n        pred_logits = model(seq_batch)\n        loss += loss_fn(pred_logits, target_batch)\n        loss.backward()\n        optimizer.step()\n        idx += 1\n        if idx == 200:\n            break\n    \n    loss = loss.item()\n    print(f'Epoch {epoch} loss: {loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:16:58.531146Z","iopub.execute_input":"2025-10-15T19:16:58.531409Z","iopub.status.idle":"2025-10-15T19:17:26.057911Z","shell.execute_reply.started":"2025-10-15T19:16:58.531389Z","shell.execute_reply":"2025-10-15T19:17:26.057146Z"}},"outputs":[{"name":"stdout","text":"Epoch 0 loss: 1.6861\nEpoch 1 loss: 1.6128\nEpoch 2 loss: 1.5505\nEpoch 3 loss: 1.5823\nEpoch 4 loss: 1.5692\nEpoch 5 loss: 1.5501\nEpoch 6 loss: 1.5963\nEpoch 7 loss: 1.5585\nEpoch 8 loss: 1.5374\nEpoch 9 loss: 1.6461\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef next_char(model, text, temperature=1):\n    txt = torch.tensor([char2int[ch] for ch in text])\n    encoded_text = txt.unsqueeze(dim=0).to(device)\n    with torch.no_grad():\n        Y_logits = model(encoded_text)\n        Y_probas = F.softmax(Y_logits[0, :, -1] / temperature, dim=-1)\n        predicted_char_id = torch.multinomial(Y_probas, num_samples=1).item()\n    return str(char_array[predicted_char_id])\n\ndef extend_text(model, text, n_chars=80, temperature=1):\n    for _ in range(n_chars):\n        text += next_char(model, text, temperature)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:17:37.519128Z","iopub.execute_input":"2025-10-15T19:17:37.519360Z","iopub.status.idle":"2025-10-15T19:17:37.524446Z","shell.execute_reply.started":"2025-10-15T19:17:37.519344Z","shell.execute_reply":"2025-10-15T19:17:37.523813Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"print(extend_text(model, \"To be or not to b\", temperature=0.4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T19:18:06.374504Z","iopub.execute_input":"2025-10-15T19:18:06.375196Z","iopub.status.idle":"2025-10-15T19:18:06.596004Z","shell.execute_reply.started":"2025-10-15T19:18:06.375172Z","shell.execute_reply":"2025-10-15T19:18:06.595486Z"}},"outputs":[{"name":"stdout","text":"To be or not to be our ears of the ender the often it the mother of heaven for the ender the worl\n","output_type":"stream"}],"execution_count":70}]}