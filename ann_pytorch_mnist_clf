{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fetching dataset from scikit learn datasets\n\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True)\nX = X.values\ny = y.astype(int).values\nprint(X.shape)\nprint(y.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:21:46.217190Z","iopub.execute_input":"2025-09-11T10:21:46.217550Z","iopub.status.idle":"2025-09-11T10:22:27.913745Z","shell.execute_reply.started":"2025-09-11T10:21:46.217527Z","shell.execute_reply":"2025-09-11T10:22:27.912796Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n","output_type":"stream"},{"name":"stdout","text":"(70000, 784)\n(70000,)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Normalizing the values\nX = ((X / 255.) - .5) * 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:23:08.198071Z","iopub.execute_input":"2025-09-11T10:23:08.198829Z","iopub.status.idle":"2025-09-11T10:23:08.458108Z","shell.execute_reply.started":"2025-09-11T10:23:08.198794Z","shell.execute_reply":"2025-09-11T10:23:08.457254Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Preparing train, valid and test set\nfrom sklearn.model_selection import train_test_split\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=10000, random_state=123, stratify=y)\nX_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)\nprint(f\"{X_train.shape} : {y_train.shape}\")\nprint(f\"{X_valid.shape} : {y_valid.shape}\")\nprint(f\"{X_test.shape} : {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-11T10:27:14.748594Z","iopub.execute_input":"2025-09-11T10:27:14.748949Z","iopub.status.idle":"2025-09-11T10:27:15.597397Z","shell.execute_reply.started":"2025-09-11T10:27:14.748926Z","shell.execute_reply":"2025-09-11T10:27:15.596498Z"}},"outputs":[{"name":"stdout","text":"(55000, 784) : (55000,)\n(5000, 784) : (5000,)\n(10000, 784) : (10000,)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Implementation of ANN using sklearn and numpy from scratch\n\nimport numpy as np\n\ndef sigmoid(z):\n    return 1. / (1. + np.exp(-z))\n\ndef int_to_onehot(y, num_labels):\n    ary = np.zeros((y.shape[0], num_labels))\n    for i, val in enumerate(y):\n        ary[i, val] = 1\n    return ary\n\nclass ANN:\n    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n        super().__init__()\n        self.num_classes = num_classes\n        \n        # hidden\n        rng = np.random.RandomState(random_seed)\n        \n        self.weight_h = rng.normal(loc=0.0, scale=0.1, size=(num_hidden, num_features))\n        self.bias_h = np.zeros(num_hidden)\n        \n        # output\n        self.weight_out = rng.normal(loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n        self.bias_out = np.zeros(num_classes)\n\n    def forward(self, x):\n        # Hidden layer\n        \n        # input dim: [n_examples, n_features]\n        #        dot [n_hidden, n_features].T\n        # output dim: [n_examples, n_hidden]\n        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n        a_h = sigmoid(z_h)\n        # Output layer\n        # input dim: [n_examples, n_hidden]\n        #        dot [n_classes, n_hidden].T\n        # output dim: [n_examples, n_classes]\n        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n        a_out = sigmoid(z_out)\n        return a_h, a_out\n\n    def backward(self, x, a_h, a_out, y):\n    \n        #########################\n        ### Output layer weights\n        #########################\n        \n        # one-hot encoding\n        y_onehot = int_to_onehot(y, self.num_classes)\n        # Part 1: dLoss/dOutWeights\n        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n        ## for convenient re-use\n        \n        # input/output dim: [n_examples, n_classes]\n        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n        # input/output dim: [n_examples, n_classes]\n        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative\n        # output dim: [n_examples, n_classes]\n        delta_out = d_loss__d_a_out * d_a_out__d_z_out\n        # gradient for output weights\n        \n        # [n_examples, n_hidden]\n        d_z_out__dw_out = a_h\n        # input dim: [n_classes, n_examples]\n        #           dot [n_examples, n_hidden]\n        # output dim: [n_classes, n_hidden]\n        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n        d_loss__db_out = np.sum(delta_out, axis=0)\n        \n        #################################\n        # Part 2: dLoss/dHiddenWeights\n        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet\n        #    * dHiddenNet/dWeight\n        \n        # [n_classes, n_hidden]\n        d_z_out__a_h = self.weight_out\n        \n        # output dim: [n_examples, n_hidden]\n        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n        \n        # [n_examples, n_hidden]\n        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n        \n        # [n_examples, n_features]\n        d_z_h__d_w_h = x\n        \n        # output dim: [n_hidden, n_features]\n        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T,\n                                d_z_h__d_w_h)\n        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n        return (d_loss__dw_out, d_loss__db_out,\n                d_loss__d_w_h, d_loss__d_b_h)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = NeuralNetMLP(num_features=28*28, num_hidden=50, num_classes=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nnum_epochs = 50\nminibatch_size = 100\ndef minibatch_generator(X, y, minibatch_size):\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    for start_idx in range(0, indices.shape[0] - minibatch_size+ 1, minibatch_size):\n        batch_idx = indices[start_idx:start_idx + minibatch_size]\n        yield X[batch_idx], y[batch_idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def mse_loss(targets, probas, num_labels=10):\n    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n    return np.mean((onehot_targets - probas)**2)\n    \ndef accuracy(targets, predicted_labels):\n    return np.mean(predicted_labels == targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n    mse, correct_pred, num_examples = 0., 0, 0\n    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n    for i, (features, targets) in enumerate(minibatch_gen):\n        _, probas = nnet.forward(features)\n        predicted_labels = np.argmax(probas, axis=1)\n        \n        loss = mse_loss(targets, probas, num_labels)\n        correct_pred += (predicted_labels == targets).sum()\n        num_examples += targets.shape[0]\n        mse += loss\n    mse = mse/i\n    acc = correct_pred/num_examples\n    return mse, acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, X_train, y_train, X_valid, y_valid, num_epochs, learning_rate=0.1):\n    epoch_loss = []\n    epoch_train_acc = []\n    epoch_valid_acc = []\n\n    for e in range(num_epochs):\n        # iterate over minibatches\n        minibatch_gen = minibatch_generator(X_train, y_train, minibatch_size)\n        for X_train_mini, y_train_mini in minibatch_gen:\n            #### Compute outputs ####\n            a_h, a_out = model.forward(X_train_mini)\n            \n            #### Compute gradients ####\n            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = model.backward(X_train_mini, a_h, a_out, y_train_mini)\n            \n            #### Update weights ####\n            model.weight_h -= learning_rate * d_loss__d_w_h\n            model.bias_h -= learning_rate * d_loss__d_b_h\n            model.weight_out -= learning_rate * d_loss__d_w_out\n            model.bias_out -= learning_rate * d_loss__d_b_out\n            \n            #### Epoch Logging ####\n            train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n            valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n            train_acc, valid_acc = train_acc*100, valid_acc*100\n\n            epoch_train_acc.append(train_acc)\n            epoch_valid_acc.append(valid_acc)\n            epoch_loss.append(train_mse)\n            \n            print(f'Epoch: {e+1:03d}/{num_epochs:03d} | Train MSE: {train_mse:.2f} | Train Acc: {train_acc:.2f}% | Valid Acc: {valid_acc:.2f}%')\n            \n            return epoch_loss, epoch_train_acc, epoch_valid_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MNIST Classification with Pytorch","metadata":{}},{"cell_type":"code","source":"import torchvision\nfrom torchvision import transforms\nimport torch\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:22:14.456027Z","iopub.execute_input":"2025-09-26T17:22:14.456284Z","iopub.status.idle":"2025-09-26T17:22:23.564452Z","shell.execute_reply.started":"2025-09-26T17:22:14.456265Z","shell.execute_reply":"2025-09-26T17:22:23.563869Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor()])\nimage_path = './'\n\nmnist_train_dataset = torchvision.datasets.MNIST(root=image_path, train=True, transform=transform, download=True)\nmnist_test_dataset = torchvision.datasets.MNIST(root=image_path, train=False, transform=transform, download=True)\nprint(mnist_train_dataset)\nprint(mnist_test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:22:25.736945Z","iopub.execute_input":"2025-09-26T17:22:25.737217Z","iopub.status.idle":"2025-09-26T17:22:27.574427Z","shell.execute_reply.started":"2025-09-26T17:22:25.737190Z","shell.execute_reply":"2025-09-26T17:22:27.573632Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 9.91M/9.91M [00:00<00:00, 50.4MB/s]\n100%|██████████| 28.9k/28.9k [00:00<00:00, 1.44MB/s]\n100%|██████████| 1.65M/1.65M [00:00<00:00, 13.4MB/s]\n100%|██████████| 4.54k/4.54k [00:00<00:00, 9.51MB/s]","output_type":"stream"},{"name":"stdout","text":"Dataset MNIST\n    Number of datapoints: 60000\n    Root location: ./\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n           )\nDataset MNIST\n    Number of datapoints: 10000\n    Root location: ./\n    Split: Test\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n           )\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_data, valid_data = torch.utils.data.random_split(mnist_train_dataset, [55_000, 5_000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:22:31.674734Z","iopub.execute_input":"2025-09-26T17:22:31.675434Z","iopub.status.idle":"2025-09-26T17:22:31.684988Z","shell.execute_reply.started":"2025-09-26T17:22:31.675402Z","shell.execute_reply":"2025-09-26T17:22:31.684424Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"batch_size = 64\ntorch.manual_seed(1)\ntrain_dl = DataLoader(train_data, batch_size, shuffle=True)\nvalid_dl = DataLoader(valid_data, batch_size, shuffle=True)\ntest_dl = DataLoader(mnist_test_dataset, batch_size, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:22:47.870755Z","iopub.execute_input":"2025-09-26T17:22:47.871340Z","iopub.status.idle":"2025-09-26T17:22:47.880569Z","shell.execute_reply.started":"2025-09-26T17:22:47.871317Z","shell.execute_reply":"2025-09-26T17:22:47.879999Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torch import nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:22:50.435094Z","iopub.execute_input":"2025-09-26T17:22:50.435619Z","iopub.status.idle":"2025-09-26T17:22:50.439326Z","shell.execute_reply.started":"2025-09-26T17:22:50.435592Z","shell.execute_reply":"2025-09-26T17:22:50.438512Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:32:29.451333Z","iopub.execute_input":"2025-09-26T17:32:29.451629Z","iopub.status.idle":"2025-09-26T17:32:29.456039Z","shell.execute_reply.started":"2025-09-26T17:32:29.451608Z","shell.execute_reply":"2025-09-26T17:32:29.455291Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class MnistClassifier(nn.Module):\n    def __init__(self, n_inputs, hidden_units):\n        super().__init__()\n        all_layers = [nn.Flatten()]\n\n        for hidden_unit in hidden_units:\n            layer = nn.Linear(n_inputs, hidden_unit)\n            all_layers.append(layer)\n            all_layers.append(nn.ReLU())\n            n_inputs = hidden_unit\n        \n        all_layers.append(nn.Linear(hidden_units[-1], 10))\n        self.model = nn.Sequential(*all_layers)\n\n    def forward(self, X):\n        return self.model(X)\n        \nhidden_units = [32, 16]\nimage_size = mnist_train_dataset[0][0].shape\ninput_size = image_size[0] * image_size[1] * image_size[2]\n\nmodel = MnistClassifier(input_size, hidden_units)\nmodel.to(device)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:32:33.877497Z","iopub.execute_input":"2025-09-26T17:32:33.878224Z","iopub.status.idle":"2025-09-26T17:32:33.886521Z","shell.execute_reply.started":"2025-09-26T17:32:33.878184Z","shell.execute_reply":"2025-09-26T17:32:33.885761Z"}},"outputs":[{"name":"stdout","text":"MnistClassifier(\n  (model): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=32, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=32, out_features=16, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=16, out_features=10, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def train(model, num_epochs, train_dl, loss_fn, optimizer):\n    for epoch in range(num_epochs):\n        accuracy_hist_train = 0.0\n        for train_X, train_y in train_dl:\n            train_X = train_X.to(device)\n            train_y = train_y.to(device)\n            predict_y = model(train_X)\n            loss_val = loss_fn(predict_y, train_y)\n            loss_val.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            is_correct = (torch.argmax(predict_y, dim=1) == train_y).float()\n            accuracy_hist_train += is_correct.sum()\n        accuracy_hist_train /= len(train_dl.dataset)\n        print(f'Epoch {epoch}  Accuracy {accuracy_hist_train:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:32:39.499843Z","iopub.execute_input":"2025-09-26T17:32:39.500095Z","iopub.status.idle":"2025-09-26T17:32:39.504957Z","shell.execute_reply.started":"2025-09-26T17:32:39.500075Z","shell.execute_reply":"2025-09-26T17:32:39.504305Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ntorch.manual_seed(1)\nnum_epochs = 20\n\ntrain(model, num_epochs, train_dl, loss_fn, optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:32:44.497241Z","iopub.execute_input":"2025-09-26T17:32:44.497823Z","iopub.status.idle":"2025-09-26T17:34:40.433836Z","shell.execute_reply.started":"2025-09-26T17:32:44.497803Z","shell.execute_reply":"2025-09-26T17:34:40.433155Z"}},"outputs":[{"name":"stdout","text":"Epoch 0  Accuracy 0.8557\nEpoch 1  Accuracy 0.9264\nEpoch 2  Accuracy 0.9414\nEpoch 3  Accuracy 0.9495\nEpoch 4  Accuracy 0.9560\nEpoch 5  Accuracy 0.9602\nEpoch 6  Accuracy 0.9648\nEpoch 7  Accuracy 0.9675\nEpoch 8  Accuracy 0.9709\nEpoch 9  Accuracy 0.9724\nEpoch 10  Accuracy 0.9742\nEpoch 11  Accuracy 0.9752\nEpoch 12  Accuracy 0.9769\nEpoch 13  Accuracy 0.9781\nEpoch 14  Accuracy 0.9793\nEpoch 15  Accuracy 0.9799\nEpoch 16  Accuracy 0.9808\nEpoch 17  Accuracy 0.9820\nEpoch 18  Accuracy 0.9830\nEpoch 19  Accuracy 0.9832\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:34:44.065531Z","iopub.execute_input":"2025-09-26T17:34:44.065770Z","iopub.status.idle":"2025-09-26T17:34:44.069175Z","shell.execute_reply.started":"2025-09-26T17:34:44.065753Z","shell.execute_reply":"2025-09-26T17:34:44.068536Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def objective(trial):\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n    n_hidden_1 = trial.suggest_int(\"n_hidden\", 20, 300)\n    n_hidden_2 = trial.suggest_int(\"n_hidden\", 20, 300)\n    model = MnistClassifier(input_size, [n_hidden_1, n_hidden_2])\n    model.to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n    train(model, 5, train_dl, loss_fn, optimizer)\n\n    model.eval()\n    with torch.no_grad():\n        accuracy_hist_valid = 0;\n        for valid_X, valid_y in valid_dl:\n            valid_X, valid_y = valid_X.to(device), valid_y.to(device)\n            y_pred_logits = model(valid_X)\n            y_pred = y_pred_logits.argmax(dim=1)\n            is_correct = (y_pred == valid_y).float()\n            accuracy_hist_valid += is_correct.sum()\n        accuracy_hist_valid /= len(valid_dl.dataset)\n    return accuracy_hist_valid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:35:50.199218Z","iopub.execute_input":"2025-09-26T17:35:50.199886Z","iopub.status.idle":"2025-09-26T17:35:50.205216Z","shell.execute_reply.started":"2025-09-26T17:35:50.199864Z","shell.execute_reply":"2025-09-26T17:35:50.204517Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"torch.manual_seed(42)\nsampler = optuna.samplers.TPESampler(seed=42)\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:35:53.283778Z","iopub.execute_input":"2025-09-26T17:35:53.284487Z","iopub.status.idle":"2025-09-26T17:38:16.288253Z","shell.execute_reply.started":"2025-09-26T17:35:53.284443Z","shell.execute_reply":"2025-09-26T17:38:16.287703Z"}},"outputs":[{"name":"stderr","text":"[I 2025-09-26 17:35:53,288] A new study created in memory with name: no-name-7367f58a-cca0-47f1-9842-bacf8690ce4b\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0  Accuracy 0.1341\nEpoch 1  Accuracy 0.1569\nEpoch 2  Accuracy 0.1979\nEpoch 3  Accuracy 0.2611\nEpoch 4  Accuracy 0.3293\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-26 17:36:21,783] Trial 0 finished with value: 0.36479997634887695 and parameters: {'learning_rate': 0.00031489116479568613, 'n_hidden': 287}. Best is trial 0 with value: 0.36479997634887695.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0  Accuracy 0.5374\nEpoch 1  Accuracy 0.8228\nEpoch 2  Accuracy 0.8800\nEpoch 3  Accuracy 0.8946\nEpoch 4  Accuracy 0.9034\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-26 17:36:50,312] Trial 1 finished with value: 0.9031999707221985 and parameters: {'learning_rate': 0.008471801418819975, 'n_hidden': 188}. Best is trial 1 with value: 0.9031999707221985.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0  Accuracy 0.1421\nEpoch 1  Accuracy 0.1449\nEpoch 2  Accuracy 0.1473\nEpoch 3  Accuracy 0.1499\nEpoch 4  Accuracy 0.1525\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-26 17:37:18,770] Trial 2 finished with value: 0.1589999943971634 and parameters: {'learning_rate': 4.207988669606632e-05, 'n_hidden': 63}. Best is trial 1 with value: 0.9031999707221985.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0  Accuracy 0.1223\nEpoch 1  Accuracy 0.1242\nEpoch 2  Accuracy 0.1263\nEpoch 3  Accuracy 0.1287\nEpoch 4  Accuracy 0.1310\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-26 17:37:47,511] Trial 3 finished with value: 0.1305999904870987 and parameters: {'learning_rate': 1.7073967431528103e-05, 'n_hidden': 263}. Best is trial 1 with value: 0.9031999707221985.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0  Accuracy 0.3255\nEpoch 1  Accuracy 0.6117\nEpoch 2  Accuracy 0.7197\nEpoch 3  Accuracy 0.7840\nEpoch 4  Accuracy 0.8229\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-09-26 17:38:16,285] Trial 4 finished with value: 0.8399999737739563 and parameters: {'learning_rate': 0.002537815508265664, 'n_hidden': 218}. Best is trial 1 with value: 0.9031999707221985.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(study.best_params)\nprint(study.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T17:38:19.659111Z","iopub.execute_input":"2025-09-26T17:38:19.659616Z","iopub.status.idle":"2025-09-26T17:38:19.663769Z","shell.execute_reply.started":"2025-09-26T17:38:19.659593Z","shell.execute_reply":"2025-09-26T17:38:19.662933Z"}},"outputs":[{"name":"stdout","text":"{'learning_rate': 0.008471801418819975, 'n_hidden': 188}\n0.9031999707221985\n","output_type":"stream"}],"execution_count":26}]}