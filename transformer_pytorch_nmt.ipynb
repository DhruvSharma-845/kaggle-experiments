{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-25T15:58:42.637290Z","iopub.execute_input":"2025-10-25T15:58:42.637531Z","iopub.status.idle":"2025-10-25T15:58:44.366783Z","shell.execute_reply.started":"2025-10-25T15:58:42.637507Z","shell.execute_reply":"2025-10-25T15:58:44.366054Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T15:59:47.308377Z","iopub.execute_input":"2025-10-25T15:59:47.308709Z","iopub.status.idle":"2025-10-25T15:59:48.968360Z","shell.execute_reply.started":"2025-10-25T15:59:47.308685Z","shell.execute_reply":"2025-10-25T15:59:48.967793Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Creating Model","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    def __init__(self, max_length, embed_dim, dropout=0.1):\n        super().__init__()\n        self.pos_embed = nn.Parameter(torch.randn(max_length, embed_dim) * 0.02)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, X):\n        return self.dropout(X + self.pos_embed[:X.size(1)])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:00:14.200275Z","iopub.execute_input":"2025-10-25T16:00:14.200735Z","iopub.status.idle":"2025-10-25T16:00:14.205497Z","shell.execute_reply.started":"2025-10-25T16:00:14.200712Z","shell.execute_reply":"2025-10-25T16:00:14.204830Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.h = num_heads\n        self.d = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, X):\n        return X.view(X.size(0), X.size(1), self.h, self.d).transpose(1, 2)\n\n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n        q = self.split_heads(self.q_proj(query))  # (B, h, Lq, d)\n        k = self.split_heads(self.k_proj(key))  # (B, h, Lk, d)\n        v = self.split_heads(self.v_proj(value))  # (B, h, Lv, d) with Lv=Lk\n        scores = q @ k.transpose(2, 3) / self.d**0.5  # (B, h, Lq, Lk)\n\n        if attn_mask is not None:\n            scores = scores.masked_fill(attn_mask, -torch.inf)  # (B, h, Lq, Lk)\n\n        if key_padding_mask is not None:\n            mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, Lk)\n            scores = scores.masked_fill(mask, -torch.inf)  # (B, h, Lq, Lk)\n        \n        weights = scores.softmax(dim=-1)  # (B, h, Lq, Lk)\n        Z = self.dropout(weights) @ v  # (B, h, Lq, d)\n        Z = Z.transpose(1, 2)  # (B, Lq, h, d)\n        Z = Z.reshape(Z.size(0), Z.size(1), self.h * self.d)  # (B, Lq, h × d)\n        return (self.out_proj(Z), weights)  # (B, Lq, h × d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:07:00.837567Z","iopub.execute_input":"2025-10-25T16:07:00.838040Z","iopub.status.idle":"2025-10-25T16:07:00.845360Z","shell.execute_reply.started":"2025-10-25T16:07:00.838018Z","shell.execute_reply":"2025-10-25T16:07:00.844505Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n        attn, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n                                 key_padding_mask=src_key_padding_mask)\n        Z = self.norm1(src + self.dropout(attn))\n        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n        return self.norm2(Z + ff)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:08:03.612714Z","iopub.execute_input":"2025-10-25T16:08:03.613267Z","iopub.status.idle":"2025-10-25T16:08:03.618543Z","shell.execute_reply.started":"2025-10-25T16:08:03.613247Z","shell.execute_reply":"2025-10-25T16:08:03.617853Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        attn1, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask)\n        Z = self.norm1(tgt + self.dropout(attn1))\n        attn2, _ = self.multihead_attn(Z, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)\n        Z = self.norm2(Z + self.dropout(attn2))\n        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n        return self.norm3(Z + ff)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:10:56.259242Z","iopub.execute_input":"2025-10-25T16:10:56.259549Z","iopub.status.idle":"2025-10-25T16:10:56.266137Z","shell.execute_reply.started":"2025-10-25T16:10:56.259531Z","shell.execute_reply":"2025-10-25T16:10:56.265355Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class NmtTransformer(nn.Module):\n    def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n                 num_heads=8, num_layers=6, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.pos_embed = PositionalEmbedding(max_length, embed_dim, dropout)\n        self.transformer = nn.Transformer(embed_dim, num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers, batch_first=True)\n        self.output = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, pair):\n        src_embeds = self.pos_embed(self.embed(pair.src_token_ids))\n        tgt_embeds = self.pos_embed(self.embed(pair.tgt_token_ids))\n        src_pad_mask = ~pair.src_mask.bool()\n        tgt_pad_mask = ~pair.tgt_mask.bool()\n        size = [pair.tgt_token_ids.size(1)] * 2\n        full_mask = torch.full(size, True, device=tgt_pad_mask.device)\n        causal_mask = torch.triu(full_mask, diagonal=1)\n        out_decoder = self.transformer(src_embeds, tgt_embeds,\n                                       src_key_padding_mask=src_pad_mask,\n                                       memory_key_padding_mask=src_pad_mask,\n                                       tgt_mask=causal_mask, tgt_is_causal=True,\n                                       tgt_key_padding_mask=tgt_pad_mask)\n        return self.output(out_decoder).permute(0, 2, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:14:47.767764Z","iopub.execute_input":"2025-10-25T16:14:47.768362Z","iopub.status.idle":"2025-10-25T16:14:47.774724Z","shell.execute_reply.started":"2025-10-25T16:14:47.768337Z","shell.execute_reply":"2025-10-25T16:14:47.773835Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:15:48.785609Z","iopub.execute_input":"2025-10-25T16:15:48.786130Z","iopub.status.idle":"2025-10-25T16:15:48.790968Z","shell.execute_reply.started":"2025-10-25T16:15:48.786106Z","shell.execute_reply":"2025-10-25T16:15:48.790082Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Preparing data","metadata":{}},{"cell_type":"code","source":"import requests \n\ndef download_data(url, save_path, chunk_size=128):\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Raise an exception for bad status codes\n\n    with open(save_path, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n    print(f\"Successfully downloaded {save_path}\")\n\ndownload_data(\"https://download.pytorch.org/tutorial/data.zip\", \"data.zip\")\n\nfrom zipfile import ZipFile\n\nextract_to_path = \"/kaggle/working/dataset\"\n\nwith ZipFile(\"/kaggle/working/data.zip\", 'r') as zip_object:\n    zip_object.extractall(path=extract_to_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:17:48.635074Z","iopub.execute_input":"2025-10-25T16:17:48.635653Z","iopub.status.idle":"2025-10-25T16:17:49.179328Z","shell.execute_reply.started":"2025-10-25T16:17:48.635630Z","shell.execute_reply":"2025-10-25T16:17:49.178651Z"}},"outputs":[{"name":"stdout","text":"Successfully downloaded data.zip\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/working/dataset/data/eng-fra.txt\", sep='\\t')\nprint(df)\nprint(df.head())\nprint(df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:18:16.191910Z","iopub.execute_input":"2025-10-25T16:18:16.192384Z","iopub.status.idle":"2025-10-25T16:18:16.579166Z","shell.execute_reply.started":"2025-10-25T16:18:16.192362Z","shell.execute_reply":"2025-10-25T16:18:16.578209Z"}},"outputs":[{"name":"stdout","text":"                                                      Go.  \\\n0                                                    Run!   \n1                                                    Run!   \n2                                                    Wow!   \n3                                                   Fire!   \n4                                                   Help!   \n...                                                   ...   \n135836  A carbon footprint is the amount of carbon dio...   \n135837  Death is something that we're often discourage...   \n135838  Since there are usually multiple websites on a...   \n135839  If someone who doesn't know your background sa...   \n135840  It may be impossible to get a completely error...   \n\n                                                     Va !  \n0                                                 Cours !  \n1                                                Courez !  \n2                                              Ça alors !  \n3                                                Au feu !  \n4                                              À l'aide !  \n...                                                   ...  \n135836  Une empreinte carbone est la somme de pollutio...  \n135837  La mort est une chose qu'on nous décourage sou...  \n135838  Puisqu'il y a de multiples sites web sur chaqu...  \n135839  Si quelqu'un qui ne connaît pas vos antécédent...  \n135840  Il est peut-être impossible d'obtenir un Corpu...  \n\n[135841 rows x 2 columns]\n     Go.        Va !\n0   Run!     Cours !\n1   Run!    Courez !\n2   Wow!  Ça alors !\n3  Fire!    Au feu !\n4  Help!  À l'aide !\n                                                      Go.  \\\ncount                                              135841   \nunique                                              93548   \ntop     I can't tell you how happy I am that you've co...   \nfreq                                                   32   \n\n                             Va !  \ncount                      135841  \nunique                     129322  \ntop     Comment cela se peut-il ?  \nfreq                            9  \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import tokenizers\n\ndef train_eng_fra():  # a generator function to iterate over all training text\n    for index, row in df.iterrows():\n        yield row.iloc[0]\n        yield row.iloc[1]\n\nmax_length = 500\nvocab_size = 10_000\nnmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\nnmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\nnmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\nnmt_tokenizer.enable_truncation(max_length=max_length)\nnmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\nnmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\nnmt_tokenizer.train_from_iterator(train_eng_fra(), nmt_tokenizer_trainer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:19:19.261752Z","iopub.execute_input":"2025-10-25T16:19:19.262409Z","iopub.status.idle":"2025-10-25T16:19:26.979678Z","shell.execute_reply.started":"2025-10-25T16:19:19.262388Z","shell.execute_reply":"2025-10-25T16:19:26.978940Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(nmt_tokenizer.decode(nmt_tokenizer.encode(\"How are you?\").ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T16:20:09.534646Z","iopub.execute_input":"2025-10-25T16:20:09.535295Z","iopub.status.idle":"2025-10-25T16:20:09.539356Z","shell.execute_reply.started":"2025-10-25T16:20:09.535272Z","shell.execute_reply":"2025-10-25T16:20:09.538626Z"}},"outputs":[{"name":"stdout","text":"How are you ?\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass TextDataset(Dataset):\n    def __init__(self, df):\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        src_text = self.df.iloc[idx, 0]\n        tgt_text = self.df.iloc[idx, 1]\n        tgt_text = f\"<s> {tgt_text} </s>\"\n        return src_text, tgt_text\n        # tgt_text = f\"<s> {tgt_text} </s>\"\n\n        # src_encodings = nmt_tokenizer.encode(src_text)\n        # tgt_encodings = nmt_tokenizer.encode(tgt_text)\n        # inputs = NmtPair(torch.tensor(src_encodings.ids), torch.tensor(src_encodings.attention_mask), \n        #                  torch.tensor(tgt_encodings.ids[:-1]), torch.tensor(tgt_encodings.attention_mask[:-1]))\n        # labels = torch.tensor(tgt_encodings.ids[1:])\n        # return inputs, labels\n\ntrain_dataset = TextDataset(df.iloc[:100000, :])\nvalid_dataset = TextDataset(df.iloc[100000:120000, :])\ntest_dataset = TextDataset(df.iloc[120000:, :])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:21:20.779794Z","iopub.execute_input":"2025-10-25T17:21:20.780057Z","iopub.status.idle":"2025-10-25T17:21:20.786226Z","shell.execute_reply.started":"2025-10-25T17:21:20.780037Z","shell.execute_reply":"2025-10-25T17:21:20.785608Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 32\n\nfrom collections import namedtuple\n\nfields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\nclass NmtPair(namedtuple(\"NmtPairBase\", fields)):\n    def to(self, device):\n        return NmtPair(self.src_token_ids.to(device), self.src_mask.to(device),\n                       self.tgt_token_ids.to(device), self.tgt_mask.to(device))\n\ndef collate_batch(batch, tokenizer = nmt_tokenizer):\n    src_text_batch, tgt_text_batch = [], []\n    for src_text, tgt_text in batch:\n        src_text_batch.append(src_text)\n        tgt_text_batch.append(tgt_text)\n    src_batch_encodings = tokenizer.encode_batch(src_text_batch)\n    tgt_batch_encodings = tokenizer.encode_batch(tgt_text_batch)\n\n    src_ids = [encoding.ids for encoding in src_batch_encodings]\n    tgt_ids = [encoding.ids[:-1] for encoding in tgt_batch_encodings]\n    src_mask = [encoding.attention_mask for encoding in src_batch_encodings]\n    tgt_mask = [encoding.attention_mask[:-1] for encoding in tgt_batch_encodings]\n    label_ids = [encoding.ids[1:] for encoding in tgt_batch_encodings]\n    \n    inputs = NmtPair(\n        torch.tensor(src_ids), \n        torch.tensor(src_mask), \n        torch.tensor(tgt_ids), \n        torch.tensor(tgt_mask)\n    )\n    labels = torch.tensor(label_ids)\n    return inputs, labels\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate_batch)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_batch)\nprint(len(train_dataloader))\nprint(len(valid_dataloader))\nprint(len(test_dataloader))\n# for data in train_dataloader:\n#     print(data)\n#     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:25:10.650997Z","iopub.execute_input":"2025-10-25T17:25:10.651604Z","iopub.status.idle":"2025-10-25T17:25:10.660388Z","shell.execute_reply.started":"2025-10-25T17:25:10.651564Z","shell.execute_reply":"2025-10-25T17:25:10.659507Z"}},"outputs":[{"name":"stdout","text":"3125\n625\n496\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"# Training model","metadata":{}},{"cell_type":"code","source":"def train_epoch(dataloader, model, optimizer, criterion):\n\n    total_loss = 0\n    for data in dataloader:\n        input_tensor, label_tensor = data\n        input_tensor, label_tensor = input_tensor.to(device), label_tensor.to(device)\n        \n        optimizer.zero_grad()\n        \n        pred = model(input_tensor)\n        \n        loss = criterion(pred, label_tensor)\n        loss.backward()\n\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:25:15.960731Z","iopub.execute_input":"2025-10-25T17:25:15.961395Z","iopub.status.idle":"2025-10-25T17:25:15.965736Z","shell.execute_reply.started":"2025-10-25T17:25:15.961370Z","shell.execute_reply":"2025-10-25T17:25:15.964873Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"from torch.optim import Adam\n\nnmt_tr_model = NmtTransformer(vocab_size, max_length, embed_dim=128, pad_id=0, num_heads=4, num_layers=2, dropout=0.1).to(device)\n# Cross Entropy\n# Input = (Batch_size,vocab_size, Seq_length)\n# Label = (Batch_size, Seq_Length)\nxentropy = nn.CrossEntropyLoss()\noptimizer = Adam(nmt_tr_model.parameters(), lr = 0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:25:18.108364Z","iopub.execute_input":"2025-10-25T17:25:18.108855Z","iopub.status.idle":"2025-10-25T17:25:18.183111Z","shell.execute_reply.started":"2025-10-25T17:25:18.108829Z","shell.execute_reply":"2025-10-25T17:25:18.182341Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def train(train_dataloader, model, n_epochs, criterion, learning_rate=0.001):\n    print_loss_total = 0  # Reset every print_every\n    nmt_tr_model.train()\n    for epoch in range(1, n_epochs + 1):\n        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n        print_loss_total += loss\n\n        if epoch % 2 == 0:\n            print_loss_avg = print_loss_total / 2\n            print_loss_total = 0\n            print('(%d %d%%) %.4f' % (epoch, epoch / n_epochs * 100, print_loss_avg))\n\ntrain(train_dataloader, nmt_tr_model, 20, xentropy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:25:27.731894Z","iopub.execute_input":"2025-10-25T17:25:27.732404Z","iopub.status.idle":"2025-10-25T17:42:56.702706Z","shell.execute_reply.started":"2025-10-25T17:25:27.732382Z","shell.execute_reply":"2025-10-25T17:42:56.702016Z"}},"outputs":[{"name":"stdout","text":"(2 10%) 1.5056\n(4 20%) 0.8834\n(6 30%) 0.7384\n(8 40%) 0.6612\n(10 50%) 0.6084\n(12 60%) 0.5731\n(14 70%) 0.5437\n(16 80%) 0.5203\n(18 90%) 0.5014\n(20 100%) 0.4845\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate(model, sentence, tokenizer, max_length = 50):\n    model.eval()\n\n    src_encodings = tokenizer.encode(sentence)\n\n    tgt_sentence = f\"<s>\"\n    index = 0\n    with torch.no_grad():\n        for index in range(max_length):\n            tgt_encodings = tokenizer.encode(tgt_sentence)\n            inputs = NmtPair(\n                torch.tensor(torch.tensor(src_encodings.ids).view(1, -1)), \n                torch.tensor(torch.tensor(src_encodings.attention_mask).view(1, -1)), \n                torch.tensor(torch.tensor(tgt_encodings.ids).view(1, -1)), \n                torch.tensor(torch.tensor(tgt_encodings.attention_mask).view(1, -1))\n            )\n\n            pred = model(inputs.to(device))\n            pred_token_ids = pred.argmax(dim=1)  # find the best token IDs\n            next_token_id = pred_token_ids[0, index]  # take the last token ID\n\n            next_token = tokenizer.id_to_token(next_token_id)\n            tgt_sentence += \" \" + next_token\n            if next_token_id == 3:\n                break\n\n    return tgt_sentence\n\nprint(evaluate(nmt_tr_model, \"I am good\", nmt_tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T17:45:30.140834Z","iopub.execute_input":"2025-10-25T17:45:30.141340Z","iopub.status.idle":"2025-10-25T17:45:30.175712Z","shell.execute_reply.started":"2025-10-25T17:45:30.141322Z","shell.execute_reply":"2025-10-25T17:45:30.174831Z"}},"outputs":[{"name":"stdout","text":"<s> Je suis bon . </s>\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/1469679525.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(torch.tensor(src_encodings.ids).view(1, -1)),\n/tmp/ipykernel_37/1469679525.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(torch.tensor(src_encodings.attention_mask).view(1, -1)),\n/tmp/ipykernel_37/1469679525.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(torch.tensor(tgt_encodings.ids).view(1, -1)),\n/tmp/ipykernel_37/1469679525.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  torch.tensor(torch.tensor(tgt_encodings.attention_mask).view(1, -1))\n","output_type":"stream"}],"execution_count":56}]}